name: Build llama.cpp 

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout this repository
      - name: Checkout repo
        uses: actions/checkout@v4

      # 2. Install dependencies
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake wget unzip git-lfs libcurl4-openssl-dev
          pip install huggingface_hub

      # 3. Build llama.cpp using CMake (disable CURL)
      - name: Build llama.cpp
        run: |
          rm -rf llama.cpp
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          mkdir build && cd build
          cmake .. -DLLAMA_CURL=OFF
          cmake --build . --config Release -j$(nproc)

      # 4. Download LLaMA 3.1 70B GGUF model
      - name: Download LLaMA 3.1 70B GGUF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          mkdir -p llama.cpp/models
          huggingface-cli download mav23/Llama-3.1-70B-Instruct-GGUF \
            --token "$HF_TOKEN" \
            --local-dir llama.cpp/models \
            --include "*.gguf"

      # 5. Test the binary with the model (optional)
      - name: Test llama.cpp build
        run: |
          cd llama.cpp
          ./build/bin/main -m models/*.gguf -p "Hello from Aiden!"

      # 6. Upload artifact (llama.cpp + model)
      - name: Upload llama.cpp + Model
        uses: actions/upload-artifact@v4
        with:
          name: llama_cpp_with_llama3_70b
          path: llama.cpp
