name: Build llama.cpp with Phi-3 Mini (CURL Enabled)

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2. Install dependencies
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake wget unzip git-lfs libcurl4-openssl-dev

      # 3. Clone and build llama.cpp with CURL enabled
      - name: Build llama.cpp
        run: |
          rm -rf llama.cpp
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          mkdir build && cd build
          cmake .. -DLLAMA_CURL=ON
          cmake --build . --config Release -j$(nproc)

      # 4. Download Phi-3 Mini GGUF model (~2.8GB)
      - name: Download Phi-3 Mini GGUF
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          mkdir -p llama.cpp/models
          cd llama.cpp/models
          wget --header="Authorization: Bearer $HF_TOKEN" \
            -O Phi-3-mini-4k-instruct-Q4_K_M.gguf \
            https://huggingface.co/bartowski/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q4_K_M.gguf

      # 5. Create ZIP for download
      - name: Create ZIP
        run: |
          cd llama.cpp
          zip -r ../llama_cpp_phi3_curl.zip .

      # 6. Upload as artifact
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama_cpp_phi3_curl
          path: llama_cpp_phi3_curl.zip
